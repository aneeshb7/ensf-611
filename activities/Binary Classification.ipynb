{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Binary Classification \n",
    "\n",
    "Follow _Introduction to Machine Learning_ [Chapter 5](https://github.com/amueller/introduction_to_ml_with_python/blob/master/05-model-evaluation-and-improvement.ipynb) **Section 5.3.2 Metrics for Binary Classification** (p.283)\n",
    "\n",
    "Binary classification:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- Confusion matrix (TP, FP, TN, FN)\n",
    "- f1 score\n",
    "- classification report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinds of errors\n",
    "\n",
    "In binary classification we have two classes. The class we are after, we usually call the *positive* class, the other *negative* class.\n",
    "\n",
    "In the heart disease dataset, we are trying to identify patients with heart disease. The *positive* class would be 'heart disease', *negative* class would be 'healthy'.\n",
    "\n",
    "**Accuracy**, the default metric we have used so far, measures how often we correctly classified an actually *positive* sample as *positive* and an actually *negative* sample as *negative*. The ideal value is 1.\n",
    "\n",
    "**False positive**: If we falsely label a sample as *positive* that in reality is *negative*. We say that a patient has heart disease, whereas in reality they do not.\n",
    "\n",
    "**False negative**: If we falsely label a sample as *negative* that in reality is *positive*. We missed identifying a patient as having heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to confusion matrix and derived metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows are actual, columns are predicted classes. The entries are:\n",
    "- True negatives TN\n",
    "- True posititives TP\n",
    "- False positives FP\n",
    "- False negatives FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_binary_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(215)\n",
    "y_true = np.random.choice(['healthy', 'sick'], 10)\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate some random predictions\n",
    "y_pred = np.random.choice(['healthy', 'sick'], 10)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true, y_pred, labels=['healthy', 'sick'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_true, y_pred, labels=['healthy', 'sick'])\n",
    "\n",
    "sns.heatmap(mat, xticklabels=['healthy', 'sick'],  yticklabels=['healthy', 'sick'], square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = mat.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate accuracy manually\n",
    "(tp+tn) / (tp+tn+fp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate precision manually\n",
    "tp / (tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_score(y_true, y_pred, pos_label='sick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate recall manually\n",
    "tp / (tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_score(y_true, y_pred, pos_label='sick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced datasets \n",
    "\n",
    "Accuracy might not be the best score for imbalanced datasets.\n",
    "\n",
    "Let's see this on a _not nine_ - _nine_ classifier on the digits dataset.\n",
    "\n",
    "A dummy classifier that always predicts the majority class (_not nine_) will achieve high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "y = digits.target == 9\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    digits.data, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5),\n",
    "                         subplot_kw={'xticks':(), 'yticks': ()})\n",
    "for ax, img in zip(axes.ravel(), digits.images):\n",
    "    ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "pred_most_frequent = dummy_majority.predict(X_val)\n",
    "print(\"Unique predicted labels: {}\".format(np.unique(pred_most_frequent)))\n",
    "print(\"Validation score: {:.2f}\".format(dummy_majority.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree does only slightly better, according to accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\n",
    "pred_tree = tree.predict(X_val)\n",
    "print(\"Validation score: {:.2f}\".format(tree.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing a _stratified_ dummy classifier to logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"stratified\").fit(X_train, y_train)\n",
    "pred_dummy = dummy.predict(X_val)\n",
    "print(\"dummy score: {:.2f}\".format(dummy.score(X_val, y_val)))\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000, C=0.1).fit(X_train, y_train)\n",
    "pred_logreg = logreg.predict(X_val)\n",
    "print(\"logreg score: {:.2f}\".format(logreg.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrices for above models\n",
    "Confusion matrices provide more detailed information.\n",
    "\n",
    "On the main diagonal, are the numbers of correctly predicted samples. The off-diagonal has the mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(y_val, pred_logreg)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_confusion_matrix_illustration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most frequent class:\")\n",
    "print(confusion_matrix(y_val, pred_most_frequent))\n",
    "print(\"\\nDummy model:\")\n",
    "print(confusion_matrix(y_val, pred_dummy))\n",
    "print(\"\\nDecision tree:\")\n",
    "print(confusion_matrix(y_val, pred_tree))\n",
    "print(\"\\nLogistic Regression\")\n",
    "print(confusion_matrix(y_val, pred_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flaw in the _most frequent_ dummy classifer is evident in the confusion matrix: predicted positive column is all zeroes, it always predicts the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy, precision, recall, f1-score\n",
    "Based on the confusion matrix, many summary metrics are computed. Here are the most important:\n",
    "\n",
    "**Accuracy:** Using the diagonal. How often are we correct?\n",
    "\n",
    "$accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$\n",
    "\n",
    "**Precision:** Using the *predicted positive* column. How many of the samples predicted positive are actually positive? Ideal precision is 1.\n",
    "\n",
    "$precision = \\frac{TP}{TP+FP}$ \n",
    "\n",
    "**Recall:** Using the *positive class* row. How many actual positive samples do we catch? Ideal recall is 1.\n",
    "\n",
    "$recall = \\frac{TP}{TP+FN}$  \n",
    "\n",
    "**_Note:_** For a given model, precision and recall are complementary. If the mistake is a false positive (FP) it cannot be a false negative (FN). As long as we make mistakes, we change the type of mistake. Hence, improving precision will reduce recall and vice-versa.\n",
    "\n",
    "**F1-score:**\n",
    "\n",
    "Combine precision and recall into one score:  \n",
    "$f_1 = 2\\cdot\\frac{precision\\cdot recall}{precision + recall}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** For the following confusion matrices, calculate precision, recall and f1 scores:\n",
    "\n",
    "```\n",
    "Format of confusion matrices is:\n",
    "[[TN  FP]\n",
    " [FN  TP]]\n",
    "\n",
    "Most frequent class:\n",
    "[[403   0]\n",
    " [ 47   0]]\n",
    "\n",
    "Dummy model:\n",
    "[[358  45]\n",
    " [ 43   4]]\n",
    "\n",
    "Decision tree:\n",
    "[[390  13]\n",
    " [ 24  23]]\n",
    "\n",
    "Logistic Regression\n",
    "[[402   1]\n",
    " [  6  41]]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "```\n",
    "Format of confusion matrices is:\n",
    "[[TN  FP]\n",
    " [FN  TP]]\n",
    "\n",
    "Most frequent class:\n",
    "[[403   0]\n",
    " [ 47   0]]\n",
    "\n",
    "precision = 0 / (0 + 0) = NaN \n",
    "recall = 0 / (0+47) = 0\n",
    "f1 = 2* NaN * 0 / (NaN+0) = 0 (or NaN?)\n",
    "\n",
    "Dummy model:\n",
    "[[358  45]\n",
    " [ 43   4]]\n",
    "\n",
    "precision = 4 / (4 + 45) = 4/49 ~ 4/50 ~ 4*0.02 = 0.08\n",
    "recall = 4 / (4+43) = 4/47 ~ 4/50 ~ 4*0.02 = 0.08\n",
    "f1 = 2*4/49 *  4/47 / (4/49 +  4/47) ~ 2* 0.08 * 0.08 / (0.08+0.08) = 0.08\n",
    "\n",
    "Decision tree:\n",
    "[[390  13]\n",
    " [ 24  23]]\n",
    "\n",
    "precision = 23 / (23 +13) = 23/36 ~ 2/3 = 0.66\n",
    "recall = 23 / (23+24) = 23/47 ~ 1/2 = 0.5\n",
    "f1 = 2*2/3 * 1/2 / (2/3 +1/2) ~ 2/3 / (4/6 + 3/6) ~ 4/6 / 7/6 = 4/7 ~ 4*0.14 = 0.56\n",
    "\n",
    "\n",
    "\n",
    "Logistic Regression\n",
    "[[402   1]\n",
    " [  6  41]]\n",
    " \n",
    " precision = 41 / (41 +1) ~ 39/40 = 0.975\n",
    "recall = 41 / (41+6) ~ 40/50 = 0.8\n",
    "f1 ~  0.9\n",
    "\n",
    "\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn `f1_score()`\n",
    "\n",
    "F1-score taking precision and recall into account is able to better distinguish between the above classifiers. It detects the flaws in both dummy classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"f1 score most frequent: {:.2f}\".format(\n",
    "    f1_score(y_val, pred_most_frequent)))\n",
    "print(\"f1 score dummy: {:.2f}\".format(f1_score(y_val, pred_dummy)))\n",
    "print(\"f1 score tree: {:.2f}\".format(f1_score(y_val, pred_tree)))\n",
    "print(\"f1 score logistic regression: {:.2f}\".format(\n",
    "    f1_score(y_val, pred_logreg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report \n",
    "\n",
    ">The classification_report function produces one line per class (here, True and False) and reports precision, recall, and f-score with this class as the positive class. Before, we assumed the minority “nine” class was the positive class. If we change the positive class to “not nine,” we can see from the output of classification_report that we obtain an f-score of 0.94 with the most_frequent model. Furthermore, for the “not nine” class we have a recall of 1, as we classified all samples as “not nine.” The last column next to the f-score provides the support of each class, which simply means the number of samples in this class according to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, pred_most_frequent,\n",
    "                            target_names=[\"not nine\", \"nine\"],\n",
    "                           zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Three additional rows in the classification report show averages of the precision, recall, and f1-score. The macro average simply computes the average across the classes, while the weighted average computes a weighted average, weighted by the number of samples in the class. Because they are averages over both classes, these metrics don’t require a notion of positive class, and in contrast to just looking at precision or just looking at recall for the positive class, averaging over both classes provides a meaningful metric in a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, pred_dummy,\n",
    "                            target_names=[\"not nine\", \"nine\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, pred_logreg,\n",
    "                            target_names=[\"not nine\", \"nine\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
