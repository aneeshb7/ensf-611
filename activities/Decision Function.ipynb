{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Function and Prediction Probability\n",
    "\n",
    "Follow _Introduction to Machine Learning_ \n",
    "- [Chapter 2](https://github.com/amueller/introduction_to_ml_with_python/blob/master/02-supervised-learning.ipynb) **Section 2.7 Uncertanty estimates from classifiers** (p.121 - 125)\n",
    "- [Chapter 5](https://github.com/amueller/introduction_to_ml_with_python/blob/master/05-model-evaluation-and-improvement.ipynb) **Section 5.3.2 Metrics for Binary Classification**  - taking uncertainty into account (p.292)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty estimates from classifiers\n",
    ">... the ability of classifiers to provide uncertainty estimates of predictions. Often, you are not only interested in which class a classifier predicts for a certain test point, but also how certain it is that this is the right class.\n",
    "\n",
    ">There are two different functions in scikit-learn that can be used to obtain uncertainty estimates from classifiers: decision_function and predict_proba. Most (but not all) classifiers have at least one of them, and many classifiers have both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_circles(noise=0.25, factor=0.5, random_state=1)\n",
    "\n",
    "# we rename the classes \"blue\" and \"red\" for illustration purposes:\n",
    "y_named = np.array([\"blue\", \"red\"])[y]\n",
    "\n",
    "# we can call train_test_split with arbitrarily many arrays;\n",
    "# all will be split in a consistent manner\n",
    "X_train, X_val, y_train_named, y_val_named, y_train, y_val = \\\n",
    "    train_test_split(X, y_named, y, random_state=0)\n",
    "\n",
    "# build the gradient boosting model\n",
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(X_train, y_train_named)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision function\n",
    "For each sample, the decision function produces a value that indicates how certain the classfier is that this sample belongs to a class.\n",
    "\n",
    "The range of the decision function can be arbitrary, and spans positive and negative values. However, by default a value of 0 determines the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_val.shape:\", X_val.shape)\n",
    "print(\"Decision function shape:\",\n",
    "      gbrt.decision_function(X_val).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first few entries of decision_function\n",
    "print(\"Decision function:\", gbrt.decision_function(X_val)[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We can recover the prediction by looking only at the sign of the decision function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Thresholded decision function:\\n\",\n",
    "      gbrt.decision_function(X_val) > 0)\n",
    "print(\"Predictions:\\n\", gbrt.predict(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">For binary classification, the “negative” class is always the first entry of the classes_ attribute, and the “positive” class is the second entry of classes_. So if you want to fully recover the output of predict, you need to make use of the classes_ attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the boolean True/False into 0 and 1\n",
    "greater_zero = (gbrt.decision_function(X_val) > 0).astype(int)\n",
    "# use 0 and 1 as indices into classes_ (classes is ['blue', 'red'])\n",
    "pred = gbrt.classes_[greater_zero]\n",
    "# pred is the same as the output of gbrt.predict\n",
    "print(\"pred is equal to predictions:\",\n",
    "      np.all(pred == gbrt.predict(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the indexing work here to recover 'red' and 'blue'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(['blue', 'red'])[np.array([0, 0, 1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the range of the decision function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_function = gbrt.decision_function(X_val)\n",
    "print(\"Decision function minimum: {:.2f} maximum: {:.2f}\".format(\n",
    "      np.min(decision_function), np.max(decision_function)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This arbitrary scaling makes the output of decision_function often hard to interpret.\n",
    "\n",
    ">In the following example we plot the decision_function for all points in the 2D plane using a color coding, next to a visualization of the decision boundary, as we saw earlier. We show training points as circles and test data as triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "mglearn.tools.plot_2d_separator(gbrt, X, ax=axes[0], alpha=.4,\n",
    "                                fill=True, cm=mglearn.cm2)\n",
    "scores_image = mglearn.tools.plot_2d_scores(gbrt, X, ax=axes[1],\n",
    "                                            alpha=.4, cm=mglearn.ReBl)\n",
    "\n",
    "for ax in axes:\n",
    "    # plot training and validation points\n",
    "    mglearn.discrete_scatter(X_val[:, 0], X_val[:, 1], y_val,\n",
    "                             markers='^', ax=ax)\n",
    "    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\n",
    "                             markers='o', ax=ax)\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "cbar = plt.colorbar(scores_image, ax=axes.tolist())\n",
    "cbar.set_alpha(1)\n",
    "axes[0].legend([\"Validation class 0\", \"Validation class 1\", \"Train class 0\",\n",
    "                \"Train class 1\"], ncol=4, loc=(.1, 1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction probability\n",
    "\n",
    "For each sample, the prediction probability indicates how certain the classfier is that this sample belongs to a class.\n",
    "\n",
    "The range of the probability is between 0 and 1. By default a probability of 0.5 determines the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of probabilities:\", gbrt.predict_proba(X_val).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get two probabilities for each sample, one for the negative and one for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first few entries of predict_proba\n",
    "print(\"Predicted probabilities:\")\n",
    "print(gbrt.predict_proba(X_val[:6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotting the probability, we choose the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "    \n",
    "mglearn.tools.plot_2d_separator(\n",
    "    gbrt, X, ax=axes[0], alpha=.4, fill=True, cm=mglearn.cm2)\n",
    "scores_image = mglearn.tools.plot_2d_scores(\n",
    "    gbrt, X, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function='predict_proba')\n",
    "\n",
    "for ax in axes:\n",
    "    # plot training and Validation points\n",
    "    mglearn.discrete_scatter(X_val[:, 0], X_val[:, 1], y_val,\n",
    "                             markers='^', ax=ax)\n",
    "    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\n",
    "                             markers='o', ax=ax)\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "# don't want a transparent colorbar\n",
    "cbar = plt.colorbar(scores_image, ax=axes.tolist())\n",
    "cbar.set_alpha(1)\n",
    "axes[0].legend([\"Validation class 0\", \"Validation class 1\", \"Train class 0\",\n",
    "                \"Train class 1\"], ncol=4, loc=(.1, 1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing decision thresholds\n",
    ">As we discussed in Chapter 2, most classifiers provide a decision_function or a predict_proba method to assess degrees of certainty about predictions. Making predictions can be seen as thresholding the output of decision_function or predict_proba at a certain fixed point—in binary classification we use 0 for the decision function and 0.5 for predict_proba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_blobs(n_samples=(400, 50), cluster_std=[7.0, 2],\n",
    "                  random_state=22)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n",
    "svc = SVC(gamma=.05).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For decision functions, positive values indicate positive class; negative values negative class by default.\n",
    "\n",
    "Applying a threshold that is lower than 0 will move the decision boundary in the direction of the negative class, hence, growing the positive class region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_decision_threshold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be decreasing the threshold, we therefore predict more positive classes. We should be able to catch more actual positives, not make false negative mistakes, and recall should go up.\n",
    "\n",
    "On the other hand, we will make more false positive mistakes now, as we are predicting more positive class samples. Precision will drop, we are less often correct.\n",
    "\n",
    "Let's have a look at the classification reports and confusion matrix with and without threshold adjustment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, svc.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_val, \n",
    "                                        svc.predict(X_val), \n",
    "                                        display_labels=['blue', 'red'],\n",
    "                                        cmap='rocket',\n",
    "                                        colorbar=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lower_threshold = svc.decision_function(X_val) > -0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, y_pred_lower_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_val, \n",
    "                                        y_pred_lower_threshold, \n",
    "                                        display_labels=['blue', 'red'],\n",
    "                                        cmap='rocket',\n",
    "                                        colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the threshold in the other direction, more positive, should have the opposite effect:\n",
    "- recall goes down\n",
    "- precision goes up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_higher_threshold = svc.decision_function(X_val) > 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, y_pred_higher_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_val, \n",
    "                                        y_pred_higher_threshold, \n",
    "                                        display_labels=['blue', 'red'],\n",
    "                                        cmap='rocket',\n",
    "                                        colorbar=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
